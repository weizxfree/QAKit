#
#  Copyright 2024 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
import mysql.connector
import os
import tiktoken
import tempfile
import json
import re
from markdown import markdown as md_to_html
import time
import difflib
try:
    from markdown_it import MarkdownIt
    from markdown_it.tree import SyntaxTreeNode
    MARKDOWN_IT_AVAILABLE = True
except ImportError:
    MARKDOWN_IT_AVAILABLE = False
    print("Warning: markdown-it-py not available. Please install with: pip install markdown-it-py")

from ...config import CONFIG, APP_CONFIG


# åˆ†å—æ¨¡å¼é…ç½®
# CHUNK_METHOD = os.getenv('CHUNK_METHOD', 'smart')  # é»˜è®¤ä½¿ç”¨ smart æ¨¡å¼


def get_configured_chunk_method():
    """è·å–é…ç½®çš„åˆ†å—æ–¹æ³•"""
    return APP_CONFIG.chunk_method


def is_dev_mode():
    """æ£€æŸ¥æ˜¯å¦å¤„äºå¼€å‘æ¨¡å¼"""
    return APP_CONFIG.dev_mode


def should_cleanup_temp_files():
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    # åœ¨devæ¨¡å¼ä¸‹ï¼Œé»˜è®¤ä¸æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä½†ç¯å¢ƒå˜é‡ä»å¯è¦†ç›–
    if is_dev_mode():
        return APP_CONFIG.cleanup_temp_files
    # åœ¨édevæ¨¡å¼ä¸‹ï¼Œé»˜è®¤æ¸…ç†ï¼Œä½†ç¯å¢ƒå˜é‡ä»å¯è¦†ç›–
    return APP_CONFIG.cleanup_temp_files


def split_markdown_to_chunks_configured(txt, chunk_token_num=256, min_chunk_tokens=10, **kwargs):
    """
    æ ¹æ®é…ç½®é€‰æ‹©åˆé€‚çš„åˆ†å—æ–¹æ³•çš„ç»Ÿä¸€æ¥å£
    
    æ”¯æŒçš„åˆ†å—æ–¹æ³•ï¼š
    - 'strict_regex': ä¸¥æ ¼æŒ‰æ­£åˆ™è¡¨è¾¾å¼åˆ†å—ï¼ˆå½“é…ç½®å¯ç”¨æ—¶ï¼‰
    - 'advanced': split_markdown_to_chunks_advanced (é«˜çº§åˆ†å—ï¼Œæ··åˆç­–ç•¥)
    - 'smart': split_markdown_to_chunks_smart (æ™ºèƒ½åˆ†å—ï¼ŒåŸºäºASTï¼Œé»˜è®¤)
    - 'basic': split_markdown_to_chunks (åŸºç¡€åˆ†å—)
    
    å¯é€šè¿‡ç¯å¢ƒå˜é‡ CHUNK_METHOD é…ç½®ï¼Œæ”¯æŒçš„å€¼ï¼šadvanced, smart, basic
    ä¹Ÿå¯é€šè¿‡kwargsä¼ å…¥è‡ªå®šä¹‰é…ç½®ï¼š
    - chunking_config: åˆ†å—é…ç½®å­—å…¸ï¼ŒåŒ…å«strategyç­‰å­—æ®µ
    """
    # æ·»åŠ è°ƒè¯•æ‰“å°
    print("=" * 80)
    print("ğŸ” [DEBUG] split_markdown_to_chunks_configured è°ƒç”¨å‚æ•°:")
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(txt) if txt else 0} å­—ç¬¦")
    print(f"ğŸ”¢ chunk_token_num: {chunk_token_num}")
    print(f"ğŸ”¢ min_chunk_tokens: {min_chunk_tokens}")
    print(f"ğŸ“‹ kwargs é”®å€¼å¯¹:")
    for key, value in kwargs.items():
        if key == 'chunking_config' and isinstance(value, dict):
            print(f"  ğŸ“Œ {key}:")
            for sub_key, sub_value in value.items():
                print(f"    ğŸ”¸ {sub_key}: {sub_value}")
        else:
            print(f"  ğŸ“Œ {key}: {value}")
    print("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„åˆ†å—é…ç½®ï¼ˆä»æ–‡æ¡£é…ç½®ä¼ å…¥ï¼‰
    custom_chunking_config = kwargs.get('chunking_config', None)
    
    if custom_chunking_config:
        print(f"ğŸ¯ [DEBUG] ä½¿ç”¨è‡ªå®šä¹‰åˆ†å—é…ç½®: {custom_chunking_config}")
        # ä½¿ç”¨æ–‡æ¡£çº§åˆ«çš„åˆ†å—é…ç½®
        strategy = custom_chunking_config.get('strategy', 'smart')
        chunk_token_num = custom_chunking_config.get('chunk_token_num', chunk_token_num)
        min_chunk_tokens = custom_chunking_config.get('min_chunk_tokens', min_chunk_tokens)
        
        print(f"ğŸš€ [DEBUG] æœ€ç»ˆåˆ†å—å‚æ•°:")
        print(f"  ğŸ“‹ ç­–ç•¥: {strategy}")
        print(f"  ğŸ”¢ åˆ†å—å¤§å°: {chunk_token_num}")
        print(f"  ğŸ”¢ æœ€å°åˆ†å—: {min_chunk_tokens}")
        
        # å…¶ä»–ç­–ç•¥çš„å¤„ç†
        if strategy == 'advanced':
            include_metadata = kwargs.pop('include_metadata', False)
            overlap_ratio = kwargs.pop('overlap_ratio', 0.0)
            print(f"  ğŸ¯ ä½¿ç”¨é«˜çº§åˆ†å—ç­–ç•¥")
            return split_markdown_to_chunks_advanced(
                txt, 
                chunk_token_num=chunk_token_num, 
                min_chunk_tokens=min_chunk_tokens,
                overlap_ratio=overlap_ratio,
                include_metadata=include_metadata
            )

        elif strategy == 'strict_regex':
            regex_pattern = custom_chunking_config.get('regex_pattern', '')
            print(f"  ğŸ¯ ä½¿ç”¨æ­£åˆ™åˆ†å—ç­–ç•¥, æ¨¡å¼: {regex_pattern}")
            if regex_pattern:
                return split_markdown_to_chunks_strict_regex(
                    txt, 
                    chunk_token_num=chunk_token_num, 
                    min_chunk_tokens=min_chunk_tokens, 
                    regex_pattern=regex_pattern
                )
            else:
                print(f"  âš ï¸ æ­£åˆ™è¡¨è¾¾å¼ä¸ºç©ºï¼Œå›é€€åˆ°æ™ºèƒ½åˆ†å—")
                # å¦‚æœæ²¡æœ‰æ­£åˆ™è¡¨è¾¾å¼ï¼Œå›é€€åˆ°æ™ºèƒ½åˆ†å—
                return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)

        elif strategy == 'smart':
            print(f"  ğŸ¯ ä½¿ç”¨æ™ºèƒ½åˆ†å—ç­–ç•¥")
            return split_markdown_to_chunks_smart(
                txt, 
                chunk_token_num=chunk_token_num, 
                min_chunk_tokens=min_chunk_tokens
            )
        elif strategy == 'basic':
            delimiter = custom_chunking_config.get('delimiter', "\n!?ã€‚ï¼›ï¼ï¼Ÿ")
            print(f"  ğŸ¯ ä½¿ç”¨åŸºç¡€åˆ†å—ç­–ç•¥, åˆ†éš”ç¬¦: {delimiter}")
            return split_markdown_to_chunks(
                txt, 
                chunk_token_num=chunk_token_num,
                delimiter=delimiter
            )
    else:
        print(f"ğŸ”„ [DEBUG] ä½¿ç”¨é»˜è®¤é…ç½® - ç¯å¢ƒå˜é‡æˆ–å›é€€åˆ°æ™ºèƒ½åˆ†å—")
        # åŸæœ‰çš„ç¯å¢ƒå˜é‡é…ç½®é€»è¾‘...
        method = get_configured_chunk_method()
        print(f"  ğŸ“Š ç¯å¢ƒé…ç½®æ–¹æ³•: {method}")
        
        if method == 'advanced':
            include_metadata = kwargs.pop('include_metadata', False)
            overlap_ratio = kwargs.pop('overlap_ratio', 0.0)
            return split_markdown_to_chunks_advanced(
                txt, 
                chunk_token_num=chunk_token_num, 
                min_chunk_tokens=min_chunk_tokens,
                overlap_ratio=overlap_ratio,
                include_metadata=include_metadata
            )
        elif method == 'basic':
            delimiter = kwargs.pop('delimiter', "\n!?ã€‚ï¼›ï¼ï¼Ÿ")
            return split_markdown_to_chunks(
                txt, 
                chunk_token_num=chunk_token_num,
                delimiter=delimiter
            )
        else:  # é»˜è®¤ä½¿ç”¨æ™ºèƒ½åˆ†å—
            return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)


def singleton(cls, *args, **kw):
    instances = {}

    def _singleton():
        key = str(cls) + str(os.getpid())
        if key not in instances:
            instances[key] = cls(*args, **kw)
        return instances[key]

    return _singleton


tiktoken_cache_dir = tempfile.gettempdir()
os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir
# encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")
encoder = tiktoken.get_encoding("cl100k_base")


def num_tokens_from_string(string: str, model_name: str = "cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    try:
        return len(encoder.encode(string))
    except Exception:
        return 0


def truncate(string: str, max_len: int) -> str:
    """Returns truncated text if the length of text exceed max_len."""
    return encoder.decode(encoder.encode(string)[:max_len])


def _extract_tables_and_remainder_md(txt: str) -> (str, list[str]):
    """
    Extracts markdown tables from text and returns the remaining text
    and a list of table strings.
    This is a simplified implementation.
    """
    lines = txt.split('\n')
    tables = []
    remainder_lines = []
    in_table = False
    current_table = []

    for line in lines:
        stripped_line = line.strip()
        # Basic check for table row (starts and ends with |)
        is_table_line = stripped_line.startswith('|') and stripped_line.endswith('|')
        # Basic check for table separator (e.g., |---|---| or |:---|:---:|)
        is_separator_line = True
        if is_table_line and '-' in stripped_line:
            parts = [p.strip() for p in stripped_line[1:-1].split('|')]
            if not all(set(p) <= set('-:') for p in parts if p): # allow only -, :
                is_separator_line = False
            if not parts: # Handles | | case
                is_separator_line = False
        else:
            is_separator_line = False


        if is_table_line or (in_table and stripped_line): # Continue table if already in it and line is not empty
            if not in_table and is_table_line and not is_separator_line: # Potential start of a new table (header)
                # Look ahead for a separator line
                next_line_index = lines.index(line) + 1
                if next_line_index < len(lines):
                    next_line_stripped = lines[next_line_index].strip()
                    next_is_separator = next_line_stripped.startswith('|') and next_line_stripped.endswith('|') and '-' in next_line_stripped
                    if next_is_separator:
                        parts_next = [p.strip() for p in next_line_stripped[1:-1].split('|')]
                        if not all(set(p) <= set('-:') for p in parts_next if p):
                            next_is_separator = False
                        if not parts_next:
                            next_is_separator = False
                    if next_is_separator:
                        in_table = True
                        current_table.append(line)
                    else: # Not a table header
                        remainder_lines.append(line)
                else: # No next line
                     remainder_lines.append(line)
            elif in_table:
                current_table.append(line)
                if not is_table_line and not stripped_line: # Empty line might end the table
                    tables.append("\n".join(current_table))
                    current_table = []
                    in_table = False
                    remainder_lines.append(line) # Add the empty line to remainder
            else: # A line that looks like a table line but isn't starting a valid table
                remainder_lines.append(line)

        elif in_table and not stripped_line : # An empty line definitely ends a table
            tables.append("\n".join(current_table))
            current_table = []
            in_table = False
            remainder_lines.append(line) # Add the empty line to remainder
        elif in_table and not is_table_line : # A non-table line also ends a table
            tables.append("\n".join(current_table))
            current_table = []
            in_table = False
            remainder_lines.append(line) # Add this line to remainder
        else:
            remainder_lines.append(line)

    if current_table: # Add any remaining table
        tables.append("\n".join(current_table))

    return "\n".join(remainder_lines), tables

def split_markdown_to_chunks(txt, chunk_token_num=128, delimiter="\n!?ã€‚ï¼›ï¼ï¼Ÿ"):
    """
    Splits markdown text into chunks, processing tables separately and merging text sections
    to be consistent with RAGFlow's naive.py markdown handling.
    """
    if not txt or not txt.strip():
        return []

    # 1. Extract tables and remainder text
    remainder_text, extracted_tables = _extract_tables_and_remainder_md(txt)
    
    processed_chunks = []
    
    # 2. Process tables: convert to HTML and add as individual chunks
    for table_md in extracted_tables:
        if table_md.strip():
            # Ensure markdown.extensions.tables is available
            try:
                table_html = md_to_html(table_md, extensions=['markdown.extensions.tables'])
                processed_chunks.append(table_html)
            except Exception as e:
                # If conversion fails, add raw table markdown as a fallback
                # Or log an error and skip
                processed_chunks.append(table_md)
                print(f"[WARNING] Failed to convert table to HTML: {e}. Added raw table markdown.")


    # 3. Initial splitting of remainder_text (non-table text)
    initial_sections = []
    if remainder_text and remainder_text.strip():
        for sec_line in remainder_text.split("\n"):
            line_content = sec_line.strip()
            if not line_content: # Keep empty lines if they are part of structure or to respect original newlines for merging
                initial_sections.append(sec_line) # Add the original line with its spacing
                continue

            if num_tokens_from_string(sec_line) > 3 * chunk_token_num:
                # Split long lines, trying to preserve original spacing if line was just very long
                mid_point = len(sec_line) // 2
                initial_sections.append(sec_line[:mid_point])
                initial_sections.append(sec_line[mid_point:])
            else:
                initial_sections.append(sec_line)
    
    # 4. Merge initial text sections into chunks respecting token limits (naive_merge logic)
    # This part needs to be careful about document order with tables.
    # The strategy here is to process text between tables.
    # However, _extract_tables_and_remainder_md might not preserve order perfectly if tables are interspersed.
    # For simplicity, we'll process all tables first, then all text. A more sophisticated approach
    # would interleave them based on original position.

    final_text_chunks = []
    current_chunk_parts = []
    current_token_count = 0

    for section_text in initial_sections:
        section_token_count = num_tokens_from_string(section_text)
        
        if not section_text.strip() and not current_chunk_parts: # Skip leading empty/whitespace sections
            continue

        if current_token_count + section_token_count <= chunk_token_num:
            current_chunk_parts.append(section_text)
            current_token_count += section_token_count
        else:
            # Finalize current_chunk if it's not empty
            if current_chunk_parts:
                final_text_chunks.append("\n".join(current_chunk_parts).strip())
            
            # Start a new chunk with the current section
            # If a single section itself is too large, it will be added as is.
            # RAGFlow's naive_merge might have more sophisticated splitting for oversized single sections.
            # For now, we add it as is or split it if it's drastically oversized.
            if section_token_count > chunk_token_num and section_token_count <= 3 * chunk_token_num: # Tolerable oversize
                 final_text_chunks.append(section_text.strip())
                 current_chunk_parts = []
                 current_token_count = 0
            elif section_token_count > 3 * chunk_token_num: # Drastically oversized, needs splitting
                # This split is basic, RAGFlow might be more nuanced
                mid = len(section_text) // 2
                final_text_chunks.append(section_text[:mid].strip())
                final_text_chunks.append(section_text[mid:].strip())
                current_chunk_parts = []
                current_token_count = 0
            else: # Start new chunk
                current_chunk_parts = [section_text]
                current_token_count = section_token_count
    
    # Add any remaining part as the last chunk
    if current_chunk_parts:
        final_text_chunks.append("\n".join(current_chunk_parts).strip())

    # Combine table HTML chunks and text chunks.
    # This simple combination appends all text chunks after all table chunks.
    # A more accurate implementation would require knowing the original order.
    # Given the current _extract_tables_and_remainder_md, this is a limitation.
    all_chunks = [chunk for chunk in processed_chunks if chunk.strip()] # Add table chunks first
    all_chunks.extend([chunk for chunk in final_text_chunks if chunk.strip()])
    
    return all_chunks


_blocks_cache = {}
def get_blocks_from_md(md_file_path):
    if md_file_path in _blocks_cache:
        return _blocks_cache[md_file_path]
    
    json_path = md_file_path.replace('.md', '_middle.json')
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
            block_list = []
            
            # æ£€æŸ¥æ•°æ®ç»“æ„ç±»å‹
            if 'pdf_info' not in data:
                print(f"[WARNING] æ— æ•ˆçš„æ•°æ®ç»“æ„: ç¼ºå°‘ pdf_info å­—æ®µ")
                _blocks_cache[md_file_path] = []
                return []
            
            for page_idx, page in enumerate(data['pdf_info']):
                # Pipelineæ¨¡å¼ï¼šæœ‰preproc_blockså­—æ®µ
                if 'preproc_blocks' in page:
                    print(f"[INFO] æ£€æµ‹åˆ°Pipelineæ¨¡å¼æ•°æ®ç»“æ„")
                    for block in page['preproc_blocks']:
                        bbox = block.get('bbox')
                        if not bbox:
                            continue
                        
                        # æå–æ–‡æœ¬å†…å®¹
                        text_content = ''
                        if 'lines' in block:
                            for line in block['lines']:
                                if 'spans' in line:
                                    for span in line['spans']:
                                        if 'content' in span:
                                            text_content += span['content']
                        
                        block_data = {
                            'bbox': bbox,
                            'type': block.get('type', 'unknown'),
                            'text': text_content.strip(),
                            'page_idx': page_idx,
                            'index': block.get('index', 0),
                            'source_mode': 'pipeline'
                        }
                        block_list.append(block_data)
                
                # VLMæ¨¡å¼ï¼šä½¿ç”¨para_blockså­—æ®µï¼ˆæ•°ç»„æ ¼å¼ï¼‰
                elif 'para_blocks' in page:
                    print(f"[INFO] æ£€æµ‹åˆ°VLMæ¨¡å¼æ•°æ®ç»“æ„")
                    para_blocks = page['para_blocks']
                    if isinstance(para_blocks, list):
                        # VLMæ¨¡å¼: para_blocksæ˜¯æ•°ç»„
                        for block in para_blocks:
                            bbox = block.get('bbox')
                            if not bbox:
                                continue
                            
                            # æå–æ–‡æœ¬å†…å®¹
                            text_content = ''
                            if 'lines' in block:
                                for line in block['lines']:
                                    if 'spans' in line:
                                        for span in line['spans']:
                                            if 'content' in span:
                                                text_content += span['content']
                            
                            block_data = {
                                'bbox': bbox,
                                'type': block.get('type', 'unknown'),
                                'text': text_content.strip(),
                                'page_idx': page_idx,
                                'index': block.get('index', 0),
                                'source_mode': 'vlm'
                            }
                            block_list.append(block_data)
                    else:
                        print(f"[WARNING] VLMæ¨¡å¼para_blocksæ ¼å¼å¼‚å¸¸ï¼ŒæœŸæœ›æ•°ç»„ä½†å¾—åˆ°: {type(para_blocks)}")
                
                else:
                    print(f"[WARNING] ç¬¬{page_idx}é¡µç¼ºå°‘preproc_blockså’Œpara_blockså­—æ®µ")
                    # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
                    possible_fields = ['blocks', 'text_blocks', 'content_blocks']
                    found = False
                    for field_name in possible_fields:
                        if field_name in page:
                            print(f"[INFO] å°è¯•ä½¿ç”¨å­—æ®µ: {field_name}")
                            found = True
                            break
                    
                    if not found:
                        print(f"[WARNING] æ— æ³•è¯†åˆ«é¡µé¢æ•°æ®ç»“æ„ï¼Œè·³è¿‡ç¬¬{page_idx}é¡µ")
            
            print(f"[INFO] ä»{json_path}æå–äº†{len(block_list)}ä¸ªå—")
            _blocks_cache[md_file_path] = block_list
            return block_list
            
    except FileNotFoundError:
        print(f"[WARNING] JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        _blocks_cache[md_file_path] = []
        return []
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSONè§£æå¤±è´¥: {e}")
        _blocks_cache[md_file_path] = []
        return []
    except Exception as e:
        print(f"[ERROR] è·å–å—åˆ—è¡¨å¤±è´¥: {e}")
        _blocks_cache[md_file_path] = []
        return []

# å…¨å±€æˆ–å¤–éƒ¨ä¼ å…¥
matched_global_indices = set()

def get_bbox_for_chunk(md_file_path, chunk_content, block_list=None, matched_global_indices=None):
    """
    æ ¹æ® md æ–‡ä»¶è·¯å¾„å’Œ chunk å†…å®¹ï¼Œè¿”å›æ„æˆè¯¥ chunk çš„è¿ç»­ block çš„ bbox åˆ—è¡¨ã€‚
    é‡‡ç”¨ difflib.SequenceMatcher æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ blockï¼ˆç›¸ä¼¼åº¦æœ€é«˜ï¼‰ï¼Œ
    ç„¶åä»è¯¥é”šç‚¹å‘å‰åæ‰©å±•ï¼Œå¯»æ‰¾åŒæ ·å­˜åœ¨äº chunk ä¸­çš„è¿ç»­ blockã€‚
    æ”¯æŒå¤–éƒ¨ä¼ å…¥ block_listï¼Œé¿å…é‡å¤è§£æã€‚
    æ”¯æŒPipelineæ¨¡å¼å’ŒVLMæ¨¡å¼çš„æ•°æ®ç»“æ„ã€‚
    åŒ¹é…åˆ°çš„å—ä¼šé€šè¿‡ matched_global_indices è®°å½•ï¼Œé¿å…åç»­ chunk é‡å¤åŒ¹é…ã€‚
    """
    try:
        if block_list is None:
            block_list = get_blocks_from_md(md_file_path)
        if matched_global_indices is None:
            matched_global_indices = set()
        if not block_list:
            print(f"[WARNING] æ— æ³•è·å–å—åˆ—è¡¨ï¼Œè·³è¿‡ä½ç½®ä¿¡æ¯è·å–")
            return None

        chunk_content_clean = chunk_content.strip()
        if not chunk_content_clean:
            return None

        # ç”¨ difflib.SequenceMatcher æ‰¾æœ€ç›¸ä¼¼çš„ block
        best_idx = -1
        best_ratio = 0.0
        for i, block in enumerate(block_list):
            if i in matched_global_indices:
                continue
            block_text = block.get('text', '').strip()
            if not block_text:
                continue
            ratio = difflib.SequenceMatcher(None, chunk_content_clean, block_text).ratio()
            if ratio > best_ratio:
                best_ratio = ratio
                best_idx = i
        if best_idx == -1 or best_ratio < 0.1:  # é˜ˆå€¼å¯è°ƒæ•´
            print(f"[WARNING] æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„å— (æœ€é«˜ç›¸ä¼¼åº¦: {best_ratio:.3f})")
            return None

        # ä»é”šç‚¹æ‰©å±•
        matched_indices = [best_idx]
        # å‘å‰æ‰©å±•
        for i in range(best_idx - 1, -1, -1):
            if i in matched_global_indices:
                continue
            block_text = block_list[i].get('text', '').strip()
            if block_text and block_text in chunk_content_clean:
                matched_indices.insert(0, i)
            else:
                break
        # å‘åæ‰©å±•
        for i in range(best_idx + 1, len(block_list)):
            if i in matched_global_indices:
                continue
            block_text = block_list[i].get('text', '').strip()
            if block_text and block_text in chunk_content_clean:
                matched_indices.append(i)
            else:
                break
        # æå–ä½ç½®ä¿¡æ¯
        positions = []
        for idx in matched_indices:
            block = block_list[idx]
            bbox = block.get('bbox')
            page_number = block.get('page_idx')
            if bbox and page_number is not None:
                position = [page_number, bbox[0], bbox[2], bbox[1], bbox[3]]
                positions.append(position)
        # è®°å½•å·²åŒ¹é… block ç´¢å¼•
        matched_global_indices.update(matched_indices)
        if positions:
            print(f"[INFO] ä¸ºchunkæ‰¾åˆ°{len(positions)}ä¸ªä½ç½®ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {best_ratio:.3f}ï¼‰ï¼Œå¹¶å·²è®°å½• matched_global_indices")
            return positions
        else:
            print(f"[WARNING] æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯")
            return None
    except Exception as e:
        print(f"[ERROR] è·å–chunkä½ç½®å¤±è´¥: {e}")
        return None


def update_document_progress(doc_id, progress=None, message=None, status=None, run=None, chunk_count=None, process_duration=None):
    """æ›´æ–°æ•°æ®åº“ä¸­æ–‡æ¡£çš„è¿›åº¦å’ŒçŠ¶æ€"""
    conn = None
    cursor = None
    try:
        from database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        updates = []
        params = []

        if progress is not None:
            updates.append("progress = %s")
            params.append(float(progress))
        if message is not None:
            updates.append("progress_msg = %s")
            params.append(message)
        if status is not None:
            updates.append("status = %s")
            params.append(status)
        if run is not None:
            updates.append("run = %s")
            params.append(run)
        if chunk_count is not None:
             updates.append("chunk_num = %s")
             params.append(chunk_count)
        if process_duration is not None:
            updates.append("process_duation = %s")
            params.append(process_duration)


        if not updates:
            return

        query = f"UPDATE document SET {', '.join(updates)} WHERE id = %s"
        params.append(doc_id)
        cursor.execute(query, params)
        conn.commit()
    except Exception as e:
        print(f"[Parser-ERROR] æ›´æ–°æ–‡æ¡£ {doc_id} è¿›åº¦å¤±è´¥: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def split_markdown_to_chunks_smart(txt, chunk_token_num=256, min_chunk_tokens=10):
    """
    åŸºäº markdown-it-py AST çš„æ™ºèƒ½åˆ†å—æ–¹æ³•ï¼Œè§£å†³ RAG Markdown æ–‡ä»¶åˆ†å—é—®é¢˜ï¼š
    1. åŸºäºè¯­ä¹‰åˆ‡åˆ†ï¼ˆä½¿ç”¨ ASTï¼‰
    2. ç»´æŠ¤è¡¨æ ¼å®Œæ•´æ€§ï¼Œå³ä½¿è¶…å‡ºäº†æœ€å¤§ tokens
    3. è€ƒè™‘ markdown çˆ¶å­åˆ†å—å…³ç³»
    """
    if not MARKDOWN_IT_AVAILABLE:
        print("Warning: markdown-it-py not available, falling back to simple chunking")
        return split_markdown_to_chunks(txt, chunk_token_num)
    
    if not txt or not txt.strip():
        return []

    # åˆå§‹åŒ– markdown-it è§£æå™¨
    md = MarkdownIt("commonmark", {"breaks": True, "html": True})
    md.enable(['table'])
    
    try:
        # è§£æä¸º AST
        tokens = md.parse(txt)
        tree = SyntaxTreeNode(tokens)
        
        # åŸºäº AST è¿›è¡Œæ™ºèƒ½åˆ†å—
        chunks = []
        current_chunk = []
        current_tokens = 0
        context_stack = []  # ç»´æŠ¤æ ‡é¢˜å±‚çº§æ ˆ
        
        for node in tree.children:
            chunk_data, should_break = _process_ast_node(
                node, context_stack, chunk_token_num, min_chunk_tokens
            )
            
            if should_break and current_chunk and current_tokens >= min_chunk_tokens:
                # å®Œæˆå½“å‰å—
                chunk_content = _finalize_ast_chunk(current_chunk, context_stack)
                if chunk_content.strip():
                    chunks.append(chunk_content)
                current_chunk = []
                current_tokens = 0
            
            if chunk_data:
                chunk_tokens = num_tokens_from_string(chunk_data)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
                if (current_tokens + chunk_tokens > chunk_token_num and 
                    current_chunk and current_tokens >= min_chunk_tokens):
                    
                    chunk_content = _finalize_ast_chunk(current_chunk, context_stack)
                    if chunk_content.strip():
                        chunks.append(chunk_content)
                    current_chunk = []
                    current_tokens = 0
                
                current_chunk.append(chunk_data)
                current_tokens += chunk_tokens
        
        # å¤„ç†æœ€åçš„å—
        if current_chunk:
            chunk_content = _finalize_ast_chunk(current_chunk, context_stack)
            if chunk_content.strip():
                chunks.append(chunk_content)
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    except Exception as e:
        print(f"AST parsing failed: {e}, falling back to simple chunking")
        return split_markdown_to_chunks(txt, chunk_token_num)


def _process_ast_node(node, context_stack, chunk_token_num, min_chunk_tokens):
    """
    å¤„ç† AST èŠ‚ç‚¹ï¼Œè¿”å› (å†…å®¹, æ˜¯å¦åº”è¯¥åˆ†å—)
    """
    node_type = node.type
    should_break = False
    content = ""
    
    if node_type == "heading":
        # æ ‡é¢˜å¤„ç†
        level = int(node.tag[1])  # h1 -> 1, h2 -> 2, etc.
        title_text = _extract_text_from_node(node)
        
        # æ›´æ–°ä¸Šä¸‹æ–‡æ ˆ
        _update_context_stack(context_stack, level, title_text)
        
        content = node.markup + " " + title_text
        should_break = True  # æ ‡é¢˜é€šå¸¸ä½œä¸ºåˆ†å—è¾¹ç•Œ
        
    elif node_type == "table":
        # è¡¨æ ¼å¤„ç† - ä¿æŒå®Œæ•´æ€§
        content = _render_table_from_ast(node)
        table_tokens = num_tokens_from_string(content)
        
        # è¡¨æ ¼è¿‡å¤§æ—¶ä¹Ÿè¦ä¿æŒå®Œæ•´æ€§
        if table_tokens > chunk_token_num:
            should_break = True
            
    elif node_type == "code_block":
        # ä»£ç å—å¤„ç†
        content = f"```{node.info or ''}\n{node.content}```"
        
    elif node_type == "blockquote":
        # å¼•ç”¨å—å¤„ç†
        content = _render_blockquote_from_ast(node)
        
    elif node_type == "list":
        # åˆ—è¡¨å¤„ç†
        content = _render_list_from_ast(node)
        
    elif node_type == "paragraph":
        # æ®µè½å¤„ç†
        content = _extract_text_from_node(node)
        
    elif node_type == "hr":
        # åˆ†éš”ç¬¦
        content = "---"
        should_break = True
        
    else:
        # å…¶ä»–ç±»å‹èŠ‚ç‚¹
        content = _extract_text_from_node(node)
    
    return content, should_break


def _update_context_stack(context_stack, level, title):
    """æ›´æ–°æ ‡é¢˜ä¸Šä¸‹æ–‡æ ˆ"""
    # ç§»é™¤æ¯”å½“å‰çº§åˆ«æ›´æ·±çš„æ ‡é¢˜
    while context_stack and context_stack[-1]['level'] >= level:
        context_stack.pop()
    
    # æ·»åŠ å½“å‰æ ‡é¢˜
    context_stack.append({'level': level, 'title': title})


def _extract_text_from_node(node):
    """ä» AST èŠ‚ç‚¹æå–æ–‡æœ¬å†…å®¹"""
    if hasattr(node, 'content') and node.content:
        return node.content
    
    text_parts = []
    if hasattr(node, 'children') and node.children:
        for child in node.children:
            if child.type == "text":
                text_parts.append(child.content)
            elif child.type == "code_inline":
                text_parts.append(f"`{child.content}`")
            elif child.type == "strong":
                text_parts.append(f"**{_extract_text_from_node(child)}**")
            elif child.type == "em":
                text_parts.append(f"*{_extract_text_from_node(child)}*")
            elif child.type == "link":
                link_text = _extract_text_from_node(child)
                text_parts.append(f"[{link_text}]({child.attrGet('href') or ''})")
            else:
                text_parts.append(_extract_text_from_node(child))
    
    return "".join(text_parts)


def _render_table_from_ast(table_node):
    """ä» AST æ¸²æŸ“è¡¨æ ¼ä¸º HTML"""
    try:
        # æ„å»ºè¡¨æ ¼çš„ markdown è¡¨ç¤º
        table_md = []
        
        for child in table_node.children:
            if child.type == "thead":
                # è¡¨å¤´å¤„ç†
                for row in child.children:
                    if row.type == "tr":
                        cells = []
                        for cell in row.children:
                            if cell.type in ["th", "td"]:
                                cells.append(_extract_text_from_node(cell))
                        table_md.append("| " + " | ".join(cells) + " |")
                
                # æ·»åŠ åˆ†éš”ç¬¦
                if table_md:
                    separator = "| " + " | ".join(["---"] * len(cells)) + " |"
                    table_md.append(separator)
                    
            elif child.type == "tbody":
                # è¡¨ä½“å¤„ç†
                for row in child.children:
                    if row.type == "tr":
                        cells = []
                        for cell in row.children:
                            if cell.type in ["th", "td"]:
                                cells.append(_extract_text_from_node(cell))
                        table_md.append("| " + " | ".join(cells) + " |")
        
        # è½¬æ¢ä¸º HTML
        table_markdown = "\n".join(table_md)
        return md_to_html(table_markdown, extensions=['markdown.extensions.tables'])
        
    except Exception as e:
        print(f"Table rendering error: {e}")
        return _extract_text_from_node(table_node)


def _render_list_from_ast(list_node):
    """ä» AST æ¸²æŸ“åˆ—è¡¨"""
    list_items = []
    list_type = list_node.attrGet('type') or 'bullet'
    
    for i, item in enumerate(list_node.children):
        if item.type == "list_item":
            item_content = _extract_text_from_node(item)
            if list_type == 'ordered':
                list_items.append(f"{i+1}. {item_content}")
            else:
                list_items.append(f"- {item_content}")
    
    return "\n".join(list_items)


def _render_blockquote_from_ast(blockquote_node):
    """ä» AST æ¸²æŸ“å¼•ç”¨å—"""
    content = _extract_text_from_node(blockquote_node)
    lines = content.split('\n')
    return '\n'.join(f"> {line}" for line in lines)


def _finalize_ast_chunk(chunk_parts, context_stack):
    """å®ŒæˆåŸºäº AST çš„ chunk æ ¼å¼åŒ–"""
    chunk_content = "\n\n".join(chunk_parts).strip()
    
    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    # ä¾‹å¦‚ï¼Œå¦‚æœchunkæ²¡æœ‰æ ‡é¢˜ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ çˆ¶çº§æ ‡é¢˜ä½œä¸ºä¸Šä¸‹æ–‡
    
    return chunk_content


def split_markdown_to_chunks_advanced(txt, chunk_token_num=256, min_chunk_tokens=10, 
                                     overlap_ratio=0.0, include_metadata=False):
    """
    åŸºäºæ ‡é¢˜å±‚çº§çš„é«˜çº§ Markdown åˆ†å—æ–¹æ³• (æ··åˆåˆ†å—ç­–ç•¥ + åŠ¨æ€é˜ˆå€¼è°ƒæ•´)
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. ä¿æŒæ ‡é¢˜ä½œä¸ºä¸»è¦åˆ†å—è¾¹ç•Œ
    2. åŠ¨æ€å¤§å°æ§åˆ¶ï¼šç›®æ ‡300-600 tokensï¼Œæœ€å¤§800 tokensï¼Œæœ€å°50 tokens  
    3. å¤„ç†è¶…å¤§åˆ†å—ï¼šåœ¨æ®µè½è¾¹ç•Œè¿›ä¸€æ­¥åˆ†å‰²
    4. å¤„ç†è¶…å°åˆ†å—ï¼šä¸ç›¸é‚»åˆ†å—åˆå¹¶
    5. ç‰¹æ®Šå†…å®¹å¤„ç†ï¼šä¿æŒè¡¨æ ¼ã€ä»£ç å—ã€å…¬å¼å®Œæ•´æ€§
    6. æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼º
    """
    if not MARKDOWN_IT_AVAILABLE:
        return split_markdown_to_chunks(txt, chunk_token_num)
    
    if not txt or not txt.strip():
        return []

    # åŠ¨æ€é˜ˆå€¼é…ç½®
    target_min_tokens = max(50, min_chunk_tokens // 2)  # æœ€å°50 tokens
    target_tokens = min(600, chunk_token_num)  # ç›®æ ‡å¤§å°ï¼š300-600 tokens
    target_max_tokens = min(800, chunk_token_num * 1.5)  # æœ€å¤§800 tokens
    
    # é…ç½®è¦ä½œä¸ºåˆ†å—è¾¹ç•Œçš„æ ‡é¢˜çº§åˆ«
    headers_to_split_on = [1, 2, 3]  # H1, H2, H3 ä½œä¸ºåˆ†å—è¾¹ç•Œ
    
    # åˆå§‹åŒ– markdown-it è§£æå™¨
    md = MarkdownIt("commonmark", {"breaks": True, "html": True})
    md.enable(['table'])
    
    try:
        # è§£æä¸º AST
        tokens = md.parse(txt)
        tree = SyntaxTreeNode(tokens)
        
        # æå–æ‰€æœ‰èŠ‚ç‚¹å’Œæ ‡é¢˜ä¿¡æ¯
        nodes_with_headers = _extract_nodes_with_header_info(tree, headers_to_split_on)
        
        # åŸºäºæ ‡é¢˜å±‚çº§è¿›è¡Œåˆæ­¥åˆ†å—
        initial_chunks = _split_by_header_levels(nodes_with_headers, headers_to_split_on)
        
        # åº”ç”¨åŠ¨æ€å¤§å°æ§åˆ¶å’Œä¼˜åŒ–
        optimized_chunks = _apply_size_control_and_optimization(
            initial_chunks, target_min_tokens, target_tokens, target_max_tokens
        )
        
        # ç”Ÿæˆæœ€ç»ˆåˆ†å—å†…å®¹
        final_chunks = []
        for chunk_info in optimized_chunks:
            content = _render_header_chunk_advanced(chunk_info)
            if content.strip():
                if include_metadata:
                    chunk_data = {
                        'content': content,
                        'metadata': chunk_info.get('headers', {}),
                        'token_count': num_tokens_from_string(content),
                        'chunk_type': chunk_info.get('chunk_type', 'header_based'),
                        'has_special_content': chunk_info.get('has_special_content', False),
                        'source_sections': chunk_info.get('source_sections', 1)
                    }
                    final_chunks.append(chunk_data)
                else:
                    final_chunks.append(content)
        
        return final_chunks
    
    except Exception as e:
        print(f"Advanced header-based parsing failed: {e}, falling back to smart chunking")
        return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)


def _apply_size_control_and_optimization(chunks, min_tokens, target_tokens, max_tokens):
    """åº”ç”¨åŠ¨æ€å¤§å°æ§åˆ¶å’Œä¼˜åŒ–ç­–ç•¥"""
    optimized_chunks = []
    
    i = 0
    while i < len(chunks):
        chunk = chunks[i]
        chunk_content = _render_header_chunk(chunk)
        chunk_tokens = num_tokens_from_string(chunk_content)
        
        # æ£€æŸ¥ç‰¹æ®Šå†…å®¹ç±»å‹
        has_special_content = _has_special_content(chunk)
        
        if chunk_tokens <= max_tokens and chunk_tokens >= min_tokens:
            # å¤§å°åˆé€‚ï¼Œç›´æ¥æ·»åŠ 
            chunk['chunk_type'] = 'normal'
            chunk['has_special_content'] = has_special_content
            optimized_chunks.append(chunk)
            
        elif chunk_tokens > max_tokens and not has_special_content:
            # è¶…å¤§åˆ†å—ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²ï¼ˆé™¤éåŒ…å«ç‰¹æ®Šå†…å®¹ï¼‰
            split_chunks = _split_oversized_chunk(chunk, target_tokens, max_tokens)
            optimized_chunks.extend(split_chunks)
            
        elif chunk_tokens < min_tokens:
            # è¶…å°åˆ†å—ï¼Œå°è¯•ä¸ä¸‹ä¸€ä¸ªåˆ†å—åˆå¹¶
            merged_chunk = _try_merge_with_next(chunk, chunks, i, target_tokens)
            if merged_chunk:
                optimized_chunks.append(merged_chunk)
                # è·³è¿‡è¢«åˆå¹¶çš„åˆ†å—
                i += merged_chunk.get('merged_count', 1) - 1
            else:
                # æ— æ³•åˆå¹¶ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡å¢å¼º
                enhanced_chunk = _enhance_small_chunk_with_context(chunk)
                optimized_chunks.append(enhanced_chunk)
        else:
            # åŒ…å«ç‰¹æ®Šå†…å®¹çš„è¶…å¤§åˆ†å—ï¼Œä¿æŒå®Œæ•´æ€§ä½†æ·»åŠ æ ‡è®°
            chunk['chunk_type'] = 'oversized_special'
            chunk['has_special_content'] = has_special_content
            optimized_chunks.append(chunk)
        
        i += 1
    
    return optimized_chunks


def _has_special_content(chunk):
    """æ£€æŸ¥åˆ†å—æ˜¯å¦åŒ…å«ç‰¹æ®Šå†…å®¹ï¼ˆè¡¨æ ¼ã€ä»£ç å—ã€å…¬å¼ç­‰ï¼‰"""
    for node_info in chunk.get('nodes', []):
        node_type = node_info.get('type', '')
        content = node_info.get('content', '')
        
        # æ£€æŸ¥ç‰¹æ®Šå†…å®¹ç±»å‹
        if node_type in ['table', 'code_block']:
            return True
        
        # æ£€æŸ¥æ•°å­¦å…¬å¼
        if '$$' in content or '$' in content:
            return True
            
        # æ£€æŸ¥HTMLè¡¨æ ¼
        if '<table>' in content and '</table>' in content:
            return True
            
    return False


def _split_oversized_chunk(chunk, target_tokens, max_tokens):
    """åˆ†å‰²è¶…å¤§åˆ†å—ï¼Œåœ¨æ®µè½è¾¹ç•Œè¿›è¡Œåˆ†å‰²"""
    split_chunks = []
    nodes = chunk.get('nodes', [])
    headers = chunk.get('headers', {})
    
    current_nodes = []
    current_tokens = 0
    
    for node_info in nodes:
        node_content = node_info.get('content', '')
        node_tokens = num_tokens_from_string(node_content)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜èŠ‚ç‚¹
        is_heading = node_info.get('type') == 'heading'
        
        # å¦‚æœå½“å‰èŠ‚ç‚¹ä¼šå¯¼è‡´è¶…å‡ºç›®æ ‡å¤§å°ï¼Œä¸”å½“å‰å·²æœ‰å†…å®¹
        if current_tokens + node_tokens > target_tokens and current_nodes:
            # åˆ›å»ºä¸€ä¸ªåˆ†å—
            new_chunk = {
                'headers': headers.copy(),
                'nodes': current_nodes.copy(),
                'chunk_type': 'split_from_oversized',
                'has_special_content': any(_has_special_content({'nodes': [n]}) for n in current_nodes)
            }
            split_chunks.append(new_chunk)
            
            # å¼€å§‹æ–°åˆ†å—
            current_nodes = [node_info]
            current_tokens = node_tokens
            
            # å¦‚æœæ˜¯æ ‡é¢˜ï¼Œæ›´æ–°headersä¸Šä¸‹æ–‡
            if is_heading:
                level = node_info.get('level', 3)
                title = node_info.get('title', '')
                new_headers = {k: v for k, v in headers.items() if k < level}
                new_headers[level] = title
                headers = new_headers
        else:
            current_nodes.append(node_info)
            current_tokens += node_tokens
            
            # æ›´æ–°æ ‡é¢˜ä¸Šä¸‹æ–‡
            if is_heading:
                level = node_info.get('level', 3)
                title = node_info.get('title', '')
                headers = {k: v for k, v in headers.items() if k < level}
                headers[level] = title
    
    # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
    if current_nodes:
        final_chunk = {
            'headers': headers.copy(),
            'nodes': current_nodes,
            'chunk_type': 'split_from_oversized',
            'has_special_content': any(_has_special_content({'nodes': [n]}) for n in current_nodes)
        }
        split_chunks.append(final_chunk)
    
    return split_chunks


def _try_merge_with_next(current_chunk, all_chunks, current_index, target_tokens):
    """å°è¯•å°†å°åˆ†å—ä¸åç»­åˆ†å—åˆå¹¶"""
    if current_index >= len(all_chunks) - 1:
        return None
    
    next_chunk = all_chunks[current_index + 1]
    
    # è®¡ç®—åˆå¹¶åçš„å¤§å°
    current_content = _render_header_chunk(current_chunk)
    next_content = _render_header_chunk(next_chunk)
    merged_tokens = num_tokens_from_string(current_content + "\n\n" + next_content)
    
    # å¦‚æœåˆå¹¶åå¤§å°åˆé€‚
    if merged_tokens <= target_tokens * 1.2:  # å…è®¸è½»å¾®è¶…å‡ºç›®æ ‡å¤§å°
        merged_chunk = {
            'headers': next_chunk.get('headers', current_chunk.get('headers', {})),
            'nodes': current_chunk.get('nodes', []) + next_chunk.get('nodes', []),
            'chunk_type': 'merged_small',
            'has_special_content': (_has_special_content(current_chunk) or 
                                  _has_special_content(next_chunk)),
            'merged_count': 2,
            'source_sections': 2
        }
        return merged_chunk
    
    return None


def _enhance_small_chunk_with_context(chunk):
    """ä¸ºå°åˆ†å—å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
    enhanced_chunk = chunk.copy()
    enhanced_chunk['chunk_type'] = 'small_enhanced'
    enhanced_chunk['has_special_content'] = _has_special_content(chunk)
    
    # ç¡®ä¿åŒ…å«è¶³å¤Ÿçš„æ ‡é¢˜ä¸Šä¸‹æ–‡
    headers = chunk.get('headers', {})
    if headers:
        # æ·»åŠ å®Œæ•´çš„æ ‡é¢˜è·¯å¾„ä½œä¸ºä¸Šä¸‹æ–‡
        context_parts = []
        for level in sorted(headers.keys()):
            context_parts.append(f"{'#' * level} {headers[level]}")
        
        # åœ¨èŠ‚ç‚¹å‰æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context_parts:
            context_node = {
                'type': 'context',
                'content': '\n'.join(context_parts),
                'headers': headers.copy(),
                'is_split_boundary': False
            }
            enhanced_chunk['nodes'] = [context_node] + enhanced_chunk.get('nodes', [])
    
    return enhanced_chunk


def _render_header_chunk_advanced(chunk_info):
    """é«˜çº§æ¸²æŸ“åŸºäºæ ‡é¢˜çš„åˆ†å—å†…å®¹ï¼ŒåŒ…å«æ›´å¥½çš„æ ¼å¼åŒ–"""
    content_parts = []
    
    # å¤„ç†æ ‡é¢˜ä¸Šä¸‹æ–‡
    chunk_has_header = any(node['type'] == 'heading' for node in chunk_info.get('nodes', []))
    headers = chunk_info.get('headers', {})
    
    # ä¸ºæŸäº›ç±»å‹çš„åˆ†å—æ·»åŠ æ ‡é¢˜ä¸Šä¸‹æ–‡
    chunk_type = chunk_info.get('chunk_type', 'normal')
    if chunk_type in ['split_from_oversized', 'small_enhanced'] and headers and not chunk_has_header:
        # æ·»åŠ æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡é¢˜
        context_header = _get_most_relevant_header_advanced(headers, chunk_type)
        if context_header:
            content_parts.append(context_header)
    
    # æ¸²æŸ“æ‰€æœ‰èŠ‚ç‚¹å†…å®¹ï¼ˆç§»é™¤æ ‡è®°ï¼Œä¿æŒå†…å®¹å¹²å‡€ï¼‰
    for node_info in chunk_info.get('nodes', []):
        if node_info.get('content', '').strip():
            content = node_info['content']
            # ç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹ï¼Œä¸æ·»åŠ ä»»ä½•æ ‡è®°
            content_parts.append(content)
    
    result = "\n\n".join(content_parts).strip()
    
    # ç§»é™¤é‡å åˆ†å—çš„æ ‡è¯†ï¼Œä¿æŒå†…å®¹å¹²å‡€
    # if chunk_type == 'overlap':
    #     result = f"[ä¸Šä¸‹æ–‡å…³è”å†…å®¹]\n{result}"
    
    return result


def _get_most_relevant_header_advanced(headers, chunk_type):
    """è·å–æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡é¢˜ï¼ˆé«˜çº§ç‰ˆæœ¬ï¼‰"""
    if not headers:
        return None
    
    # æ ¹æ®åˆ†å—ç±»å‹é€‰æ‹©ä¸åŒçš„ä¸Šä¸‹æ–‡ç­–ç•¥
    if chunk_type == 'split_from_oversized':
        # åˆ†å‰²åˆ†å—ï¼šæ˜¾ç¤ºæœ€æ·±å±‚çº§çš„æ ‡é¢˜
        max_level = max(headers.keys())
        return f"{'#' * max_level} {headers[max_level]}"
    
    elif chunk_type in ['small_enhanced']:
        # å¢å¼ºåˆ†å—ï¼šæ˜¾ç¤ºæœ€ç›¸å…³çš„æ ‡é¢˜
        max_level = max(headers.keys())
        return f"{'#' * max_level} {headers[max_level]}"
    
    else:
        # æ™®é€šåˆ†å—ï¼šæ˜¾ç¤ºæœ€ç›¸å…³çš„æ ‡é¢˜
        max_level = max(headers.keys())
        return f"{'#' * max_level} {headers[max_level]}"


def optimize_chunks_for_rag(chunks, target_vector_dim=1536):
    """
    åŸºç¡€RAGåˆ†å—ä¼˜åŒ–ï¼Œä¸ºå‘é‡åŒ–åšå‡†å¤‡
    """
    optimized_chunks = []
    
    for chunk_data in chunks:
        if isinstance(chunk_data, str):
            chunk_data = {'content': chunk_data, 'token_count': num_tokens_from_string(chunk_data)}
        
        optimized_chunks.append(chunk_data)
    
    return optimized_chunks

def _extract_nodes_with_header_info(tree, headers_to_split_on):
    """æå–æ‰€æœ‰èŠ‚ç‚¹åŠå…¶å¯¹åº”çš„æ ‡é¢˜ä¿¡æ¯"""
    nodes_with_headers = []
    current_headers = {}  # å½“å‰çš„æ ‡é¢˜å±‚çº§è·¯å¾„
    
    for node in tree.children:
        if node.type == "heading":
            level = int(node.tag[1])  # h1 -> 1, h2 -> 2, etc.
            title = _extract_text_from_node(node)
            
            # æ›´æ–°å½“å‰æ ‡é¢˜è·¯å¾„
            # ç§»é™¤æ¯”å½“å‰çº§åˆ«æ›´æ·±çš„æ ‡é¢˜
            current_headers = {k: v for k, v in current_headers.items() if k < level}
            # æ·»åŠ å½“å‰æ ‡é¢˜
            current_headers[level] = title
            
            # å¦‚æœæ˜¯åˆ†å—è¾¹ç•Œæ ‡é¢˜ï¼Œæ ‡è®°ä¸ºåˆ†å—èµ·å§‹ç‚¹
            is_split_boundary = level in headers_to_split_on
            
            nodes_with_headers.append({
                'node': node,
                'type': 'heading',
                'level': level,
                'title': title,
                'headers': current_headers.copy(),
                'is_split_boundary': is_split_boundary,
                'content': node.markup + " " + title
            })
        else:
            # éæ ‡é¢˜èŠ‚ç‚¹
            content = _render_node_content(node)
            if content.strip():
                nodes_with_headers.append({
                    'node': node,
                    'type': node.type,
                    'headers': current_headers.copy(),
                    'is_split_boundary': False,
                    'content': content
                })
    
    return nodes_with_headers


def _render_node_content(node):
    """æ¸²æŸ“å•ä¸ªèŠ‚ç‚¹çš„å†…å®¹"""
    if node.type == "table":
        return _render_table_from_ast(node)
    elif node.type == "code_block":
        return f"```{node.info or ''}\n{node.content}```"
    elif node.type == "blockquote":
        return _render_blockquote_from_ast(node)
    elif node.type in ["bullet_list", "ordered_list"]:
        return _render_list_from_ast(node)
    elif node.type == "paragraph":
        return _extract_text_from_node(node)
    elif node.type == "hr":
        return "---"
    else:
        return _extract_text_from_node(node)


def _split_by_header_levels(nodes_with_headers, headers_to_split_on):
    """åŸºäºæ ‡é¢˜å±‚çº§è¿›è¡Œåˆ†å—ï¼Œæ™ºèƒ½å¤„ç†è¿ç»­æ ‡é¢˜"""
    chunks = []
    current_chunk = {
        'headers': {},
        'nodes': []
    }
    
    i = 0
    while i < len(nodes_with_headers):
        node_info = nodes_with_headers[i]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å—è¾¹ç•Œæ ‡é¢˜
        if node_info['is_split_boundary']:
            # å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºè¿ç»­çŸ­æ ‡é¢˜çš„æƒ…å†µ
            if node_info['type'] == 'heading':
                current_title = node_info.get('title', '').strip()
                
                # æ£€æŸ¥å½“å‰æ ‡é¢˜æ˜¯å¦å¾ˆçŸ­ï¼ˆå¯èƒ½åªæ˜¯ç¼–å·ï¼‰
                is_short_title = (
                    len(current_title) <= 12 and 
                    (
                        # çº¯æ•°å­—ç¼–å·å¦‚ "3.7", "4.1"
                        (current_title.replace('.', '').replace(' ', '').isdigit()) or
                        # çŸ­ç¼–å·å¦‚ "3.7", "4", "A.1"  
                        (len(current_title.split()) <= 2 and 
                         any(char.isdigit() for char in current_title))
                    )
                )
                
                # å¦‚æœæ˜¯çŸ­æ ‡é¢˜ï¼Œå‘å‰æŸ¥æ‰¾çœ‹æ˜¯å¦æœ‰ç´§è·Ÿçš„å†…å®¹æ ‡é¢˜
                if is_short_title:
                    # æŸ¥æ‰¾æ¥ä¸‹æ¥çš„å‡ ä¸ªèŠ‚ç‚¹ï¼Œçœ‹æ˜¯å¦æœ‰å®è´¨æ€§å†…å®¹æ ‡é¢˜
                    found_content_header = False
                    j = i + 1
                    
                    # å‘å‰æŸ¥çœ‹æœ€å¤š3ä¸ªèŠ‚ç‚¹
                    while j < len(nodes_with_headers) and j < i + 4:
                        next_node = nodes_with_headers[j]
                        
                        # å¦‚æœæ‰¾åˆ°å¦ä¸€ä¸ªæ ‡é¢˜
                        if next_node.get('type') == 'heading':
                            next_title = next_node.get('title', '').strip()
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ›´æœ‰å®è´¨å†…å®¹çš„æ ‡é¢˜
                            is_content_header = (
                                len(next_title) > 12 or  # è¾ƒé•¿çš„æ ‡é¢˜
                                (len(next_title.split()) > 2) or  # å¤šä¸ªè¯
                                any(word for word in next_title.split() 
                                    if len(word) > 3 and not word.replace('.', '').isdigit())  # æœ‰éæ•°å­—è¯æ±‡
                            )
                            
                            if is_content_header:
                                found_content_header = True
                                break
                        
                        # å¦‚æœé‡åˆ°å…¶ä»–å†…å®¹ï¼Œåœæ­¢æŸ¥æ‰¾
                        elif next_node.get('content', '').strip():
                            break
                        
                        j += 1
                    
                    # å¦‚æœæ‰¾åˆ°äº†å†…å®¹æ ‡é¢˜ï¼Œè·³è¿‡å½“å‰æ ‡é¢˜çš„åˆ†å—å¤„ç†
                    if found_content_header:
                        # ç›´æ¥æ·»åŠ åˆ°å½“å‰å—ï¼Œä¸ä½œä¸ºåˆ†å—è¾¹ç•Œ
                        current_chunk['nodes'].append(node_info)
                        i += 1
                        continue
            
            # æ­£å¸¸çš„åˆ†å—è¾¹ç•Œå¤„ç†
            # å®Œæˆå½“å‰å—ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
            if (current_chunk['nodes'] and 
                any(n for n in current_chunk['nodes'] if n['content'].strip())):
                chunks.append(current_chunk)
                current_chunk = {
                    'headers': {},
                    'nodes': []
                }
        
        # æ›´æ–°å½“å‰å—çš„æ ‡é¢˜ä¿¡æ¯
        if node_info['headers']:
            current_chunk['headers'] = node_info['headers'].copy()
        
        # æ·»åŠ èŠ‚ç‚¹åˆ°å½“å‰å—
        current_chunk['nodes'].append(node_info)
        i += 1
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk['nodes'] and any(n for n in current_chunk['nodes'] if n['content'].strip()):
        chunks.append(current_chunk)
    
    return chunks


def _render_header_chunk(chunk_info):
    """æ¸²æŸ“åŸºäºæ ‡é¢˜çš„åˆ†å—å†…å®¹ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰"""
    content_parts = []
    
    # æ·»åŠ æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœåˆ†å—æœ¬èº«ä¸åŒ…å«æ ‡é¢˜ï¼‰
    chunk_has_header = any(node['type'] == 'heading' for node in chunk_info.get('nodes', []))
    
    if not chunk_has_header and chunk_info.get('headers'):
        # æ·»åŠ æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡é¢˜
        context_header = _get_most_relevant_header(chunk_info['headers'])
        if context_header:
            content_parts.append(context_header)
    
    # æ¸²æŸ“æ‰€æœ‰èŠ‚ç‚¹å†…å®¹
    for node_info in chunk_info.get('nodes', []):
        if node_info.get('content', '').strip():
            content_parts.append(node_info['content'])
    
    return "\n\n".join(content_parts).strip()


def _get_most_relevant_header(headers):
    """è·å–æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡é¢˜ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰"""
    if not headers:
        return None
    
    # é€‰æ‹©æœ€æ·±å±‚çº§çš„æ ‡é¢˜ä½œä¸ºä¸Šä¸‹æ–‡
    max_level = max(headers.keys())
    return f"{'#' * max_level} {headers[max_level]}"


def split_markdown_to_chunks_strict_regex(txt, chunk_token_num=256, min_chunk_tokens=10, regex_pattern=''):
    """
    ä½¿ç”¨è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸¥æ ¼åˆ†å—
    
    Args:
        txt: è¦åˆ†å—çš„æ–‡æœ¬
        chunk_token_num: ç›®æ ‡åˆ†å—å¤§å°ï¼ˆtokensï¼‰
        min_chunk_tokens: æœ€å°åˆ†å—å¤§å°ï¼ˆtokensï¼‰
        regex_pattern: è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼
        
    Returns:
        åˆ†å—åˆ—è¡¨
    """
    if not txt or not txt.strip():
        return []
    
    if not regex_pattern or not regex_pattern.strip():
        print(f"âš ï¸ [WARNING] æ­£åˆ™è¡¨è¾¾å¼ä¸ºç©ºï¼Œå›é€€åˆ°æ™ºèƒ½åˆ†å—")
        return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)
    
    try:
        print(f"ğŸ¯ [DEBUG] ä½¿ç”¨è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆ†å—: {regex_pattern}")
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•ï¼šé€è¡Œå¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªåŒ¹é…éƒ½å¼€å§‹æ–°åˆ†å—
        # ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåªåŒ¹é…è¡Œå¼€å¤´æˆ–å‰é¢åªæœ‰ç©ºæ ¼çš„æ¡æ–‡
        precise_pattern = r'^\s*' + regex_pattern
        
        lines = txt.split('\n')
        chunks = []
        current_chunk = []
        
        for line in lines:
            # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦ä»¥æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¼€å§‹ï¼ˆçœŸæ­£çš„æ¡æ–‡å¼€å§‹ï¼‰
            if re.search(precise_pattern, line) and current_chunk:
                # å¦‚æœå½“å‰è¡ŒåŒ…å«åŒ¹é…ä¸”å½“å‰å·²æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰åˆ†å—
                chunk_content = '\n'.join(current_chunk).strip()
                if chunk_content:
                    chunks.append(chunk_content)
                
                # å¼€å§‹æ–°åˆ†å—
                current_chunk = [line]
            else:
                # å°†å½“å‰è¡Œæ·»åŠ åˆ°å½“å‰åˆ†å—
                current_chunk.append(line)
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk:
            chunk_content = '\n'.join(current_chunk).strip()
            if chunk_content:
                chunks.append(chunk_content)
        
        # è¿‡æ»¤å’Œç»Ÿè®¡
        final_chunks = [chunk for chunk in chunks if chunk.strip()]
        
        print(f"ğŸ“Š [DEBUG] æ­£åˆ™åˆ†å—ç»“æœ: {len(final_chunks)} ä¸ªåˆ†å—")
        if final_chunks:
            token_counts = [num_tokens_from_string(chunk) for chunk in final_chunks]
            print(f"ğŸ“ˆ [DEBUG] Tokenåˆ†å¸ƒ: {min(token_counts)}-{max(token_counts)} (å¹³å‡: {sum(token_counts)/len(token_counts):.1f})")
        
        return final_chunks
        
    except re.error as e:
        print(f"âŒ [ERROR] è‡ªå®šä¹‰æ­£åˆ™åˆ†å—å¤±è´¥ï¼Œæ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}ï¼Œå›é€€åˆ°æ™ºèƒ½åˆ†å—")
        return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)
    except Exception as e:
        print(f"âŒ [ERROR] è‡ªå®šä¹‰æ­£åˆ™åˆ†å—å‘ç”Ÿå¼‚å¸¸: {e}ï¼Œå›é€€åˆ°æ™ºèƒ½åˆ†å—")
        return split_markdown_to_chunks_smart(txt, chunk_token_num, min_chunk_tokens)