#!/usr/bin/env python3
"""
KnowFlow æ‰¹é‡ Chunk æ·»åŠ  API æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°å¢çš„ POST /datasets/<dataset_id>/documents/<document_id>/chunks/batch æ¥å£
"""

import requests
import json
import time
from typing import List, Dict, Optional

class KnowFlowBatchTester:
    def __init__(self, base_url: str = "http://localhost:9380", api_key: str = ""):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            base_url: RAGFlow æœåŠ¡çš„åŸºç¡€URL
            api_key: APIå¯†é’¥
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}' if api_key else ''
        }
    
    def test_batch_add_chunks(self, 
                             dataset_id: str, 
                             document_id: str, 
                             chunks: List[Dict],
                             batch_size: Optional[int] = None) -> Dict:
        """
        æµ‹è¯•æ‰¹é‡æ·»åŠ  chunks
        
        Args:
            dataset_id: æ•°æ®é›†ID
            document_id: æ–‡æ¡£ID
            chunks: chunkæ•°æ®åˆ—è¡¨
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            
        Returns:
            APIå“åº”ç»“æœ
        """
        url = f"{self.base_url}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks/batch"
        
        payload = {"chunks": chunks}
        if batch_size:
            payload["batch_size"] = batch_size
        
        print(f"ğŸš€ å‘é€æ‰¹é‡è¯·æ±‚åˆ°: {url}")
        print(f"ğŸ“Š æ‰¹é‡å¤§å°: {len(chunks)} chunks")
        if batch_size:
            print(f"ğŸ”§ å¤„ç†åˆ†ç‰‡: {batch_size}")
        
        try:
            start_time = time.time()
            response = requests.post(url, headers=self.headers, json=payload, timeout=60)
            end_time = time.time()
            
            print(f"â±ï¸  è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"ğŸ“¤ HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                self._print_success_result(result)
                return result
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥:")
                print(f"   çŠ¶æ€ç : {response.status_code}")
                print(f"   å“åº”: {response.text}")
                return {"error": response.text}
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
            return {"error": str(e)}
    
    def _print_success_result(self, result: Dict):
        """æ‰“å°æˆåŠŸç»“æœçš„ç»Ÿè®¡ä¿¡æ¯"""
        data = result.get('data', {})
        
        print("âœ… æ‰¹é‡æ·»åŠ æˆåŠŸ!")
        print(f"   âœ… æˆåŠŸæ·»åŠ : {data.get('total_added', 0)} chunks")
        
        if data.get('total_failed', 0) > 0:
            print(f"   âŒ å¤±è´¥æ•°é‡: {data.get('total_failed', 0)} chunks")
        
        # å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        stats = data.get('processing_stats', {})
        if stats:
            print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   ğŸ“¥ è¯·æ±‚æ€»æ•°: {stats.get('total_requested', 0)}")
            print(f"   ğŸ”„ åˆ†ç‰‡å¤§å°: {stats.get('batch_size_used', 0)}")
            print(f"   ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {stats.get('batches_processed', 0)}")
            print(f"   ğŸ’° åµŒå…¥æˆæœ¬: {stats.get('embedding_cost', 0)}")
            
            errors = stats.get('processing_errors')
            if errors:
                print(f"   âš ï¸  å¤„ç†é”™è¯¯: {len(errors)} ä¸ª")
                for error in errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"      - {error}")
    
    def create_test_chunks(self, count: int = 10, content_prefix: str = "æµ‹è¯•å†…å®¹") -> List[Dict]:
        """
        åˆ›å»ºæµ‹è¯•ç”¨çš„ chunk æ•°æ®
        
        Args:
            count: åˆ›å»ºçš„chunkæ•°é‡
            content_prefix: å†…å®¹å‰ç¼€
            
        Returns:
            chunkæ•°æ®åˆ—è¡¨
        """
        chunks = []
        for i in range(count):
            chunk = {
                "content": f"{content_prefix} {i+1} - è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•KnowFlowæ‰¹é‡æ·»åŠ åŠŸèƒ½çš„ç¤ºä¾‹æ–‡æœ¬å†…å®¹ã€‚åŒ…å«äº†ä¸€äº›æœ‰æ„ä¹‰çš„ä¿¡æ¯ç”¨äºæµ‹è¯•å‘é‡åŒ–å’Œæœç´¢åŠŸèƒ½ã€‚",
                "important_keywords": [f"å…³é”®è¯{i+1}", f"æµ‹è¯•{i+1}", "KnowFlow", "æ‰¹é‡å¤„ç†"],
                "questions": [
                    f"ä»€ä¹ˆæ˜¯{content_prefix} {i+1}ï¼Ÿ",
                    f"å¦‚ä½•ä½¿ç”¨{content_prefix} {i+1}ï¼Ÿ"
                ]
            }
            chunks.append(chunk)
        return chunks
    
    def create_test_chunks_with_positions(self, count: int = 5, content_prefix: str = "ä½ç½®æµ‹è¯•") -> List[Dict]:
        """
        åˆ›å»ºåŒ…å«ä½ç½®ä¿¡æ¯çš„æµ‹è¯• chunk æ•°æ®
        
        Args:
            count: åˆ›å»ºçš„chunkæ•°é‡
            content_prefix: å†…å®¹å‰ç¼€
            
        Returns:
            åŒ…å«ä½ç½®ä¿¡æ¯çš„chunkæ•°æ®åˆ—è¡¨
        """
        chunks = []
        for i in range(count):
            chunk = {
                "content": f"{content_prefix} {i+1} - è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä½ç½®ä¿¡æ¯çš„æµ‹è¯•æ–‡æœ¬å†…å®¹ã€‚ä½ç½®ä¿¡æ¯å¯ä»¥å¸®åŠ©å®šä½æ–‡æœ¬åœ¨åŸå§‹æ–‡æ¡£ä¸­çš„å…·ä½“ä½ç½®ã€‚",
                "important_keywords": [f"ä½ç½®{i+1}", f"æµ‹è¯•{i+1}", "KnowFlow", "ä½ç½®ä¿¡æ¯"],
                "questions": [
                    f"ä»€ä¹ˆæ˜¯{content_prefix} {i+1}çš„ä½ç½®ï¼Ÿ",
                    f"å¦‚ä½•ä½¿ç”¨ä½ç½®ä¿¡æ¯ï¼Ÿ"
                ],
                "positions": [
                    [i+1, 100 + i*50, 500 + i*50, 200 + i*30, 250 + i*30],  # [page_num, left, right, top, bottom]
                    [i+1, 100 + i*50, 500 + i*50, 260 + i*30, 310 + i*30]   # å¯ä»¥æœ‰å¤šä¸ªä½ç½®
                ]
            }
            chunks.append(chunk)
        return chunks
    
    def test_batch_add_chunks_with_positions(self, 
                                           dataset_id: str, 
                                           document_id: str, 
                                           chunks: List[Dict],
                                           batch_size: Optional[int] = None) -> Dict:
        """
        æµ‹è¯•æ‰¹é‡æ·»åŠ åŒ…å«ä½ç½®ä¿¡æ¯çš„ chunks
        
        Args:
            dataset_id: æ•°æ®é›†ID
            document_id: æ–‡æ¡£ID
            chunks: chunkæ•°æ®åˆ—è¡¨ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            
        Returns:
            APIå“åº”ç»“æœ
        """
        url = f"{self.base_url}/api/v1/datasets/{dataset_id}/documents/{document_id}/chunks/batch"
        
        payload = {"chunks": chunks}
        if batch_size:
            payload["batch_size"] = batch_size
        
        print(f"ğŸš€ å‘é€åŒ…å«ä½ç½®ä¿¡æ¯çš„æ‰¹é‡è¯·æ±‚åˆ°: {url}")
        print(f"ğŸ“Š æ‰¹é‡å¤§å°: {len(chunks)} chunks (åŒ…å«ä½ç½®ä¿¡æ¯)")
        if batch_size:
            print(f"ğŸ”§ å¤„ç†åˆ†ç‰‡: {batch_size}")
        
        # æ˜¾ç¤ºä½ç½®ä¿¡æ¯ç¤ºä¾‹
        if chunks and "positions" in chunks[0]:
            print(f"ğŸ“ ä½ç½®ä¿¡æ¯ç¤ºä¾‹: {chunks[0]['positions']}")
        
        try:
            start_time = time.time()
            response = requests.post(url, headers=self.headers, json=payload, timeout=60)
            end_time = time.time()
            
            print(f"â±ï¸  è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"ğŸ“¤ HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                self._print_success_result_with_positions(result)
                return result
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥:")
                print(f"   çŠ¶æ€ç : {response.status_code}")
                print(f"   å“åº”: {response.text}")
                return {"error": response.text}
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
            return {"error": str(e)}
    
    def _print_success_result_with_positions(self, result: Dict):
        """æ‰“å°åŒ…å«ä½ç½®ä¿¡æ¯çš„æˆåŠŸç»“æœç»Ÿè®¡"""
        data = result.get('data', {})
        
        print("âœ… æ‰¹é‡æ·»åŠ æˆåŠŸ!")
        print(f"   âœ… æˆåŠŸæ·»åŠ : {data.get('total_added', 0)} chunks")
        
        if data.get('total_failed', 0) > 0:
            print(f"   âŒ å¤±è´¥æ•°é‡: {data.get('total_failed', 0)} chunks")
        
        # æ£€æŸ¥è¿”å›çš„ chunks æ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
        chunks = data.get('chunks', [])
        if chunks:
            chunks_with_positions = [c for c in chunks if c.get('positions')]
            print(f"   ğŸ“ åŒ…å«ä½ç½®ä¿¡æ¯çš„chunks: {len(chunks_with_positions)}/{len(chunks)}")
            
            if chunks_with_positions:
                print(f"   ğŸ“ ä½ç½®ä¿¡æ¯ç¤ºä¾‹: {chunks_with_positions[0]['positions'][:2]}...")  # åªæ˜¾ç¤ºå‰2ä¸ªä½ç½®
        
        # å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        stats = data.get('processing_stats', {})
        if stats:
            print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   ğŸ“¥ è¯·æ±‚æ€»æ•°: {stats.get('total_requested', 0)}")
            print(f"   ğŸ”„ åˆ†ç‰‡å¤§å°: {stats.get('batch_size_used', 0)}")
            print(f"   ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {stats.get('batches_processed', 0)}")
            print(f"   ğŸ’° åµŒå…¥æˆæœ¬: {stats.get('embedding_cost', 0)}")
            
            errors = stats.get('processing_errors')
            if errors:
                print(f"   âš ï¸  å¤„ç†é”™è¯¯: {len(errors)} ä¸ª")
                for error in errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"      - {error}")
    
    def run_test_suite(self, dataset_id: str, document_id: str):
        """
        è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
        
        Args:
            dataset_id: æ•°æ®é›†ID
            document_id: æ–‡æ¡£ID
        """
        print("ğŸ§ª KnowFlow æ‰¹é‡ Chunk æ·»åŠ æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        # æµ‹è¯•1: å°æ‰¹é‡æµ‹è¯• (5ä¸ªchunk)
        print("\nğŸ“‹ æµ‹è¯•1: å°æ‰¹é‡æµ‹è¯• (5 chunks)")
        print("-" * 40)
        small_chunks = self.create_test_chunks(5, "å°æ‰¹é‡æµ‹è¯•")
        result1 = self.test_batch_add_chunks(dataset_id, document_id, small_chunks, batch_size=2)
        
        time.sleep(2)  # ç­‰å¾…2ç§’
        
        # æµ‹è¯•2: ä¸­ç­‰æ‰¹é‡æµ‹è¯• (20ä¸ªchunk)
        print("\nğŸ“‹ æµ‹è¯•2: ä¸­ç­‰æ‰¹é‡æµ‹è¯• (20 chunks)")
        print("-" * 40)
        medium_chunks = self.create_test_chunks(20, "ä¸­ç­‰æ‰¹é‡æµ‹è¯•")
        result2 = self.test_batch_add_chunks(dataset_id, document_id, medium_chunks, batch_size=5)
        
        time.sleep(2)  # ç­‰å¾…2ç§’
        
        # æµ‹è¯•3: å¤§æ‰¹é‡æµ‹è¯• (50ä¸ªchunk)
        print("\nğŸ“‹ æµ‹è¯•3: å¤§æ‰¹é‡æµ‹è¯• (50 chunks)")
        print("-" * 40)
        large_chunks = self.create_test_chunks(50, "å¤§æ‰¹é‡æµ‹è¯•")
        result3 = self.test_batch_add_chunks(dataset_id, document_id, large_chunks, batch_size=10)
        
        # æµ‹è¯•æ€»ç»“
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        
        tests = [
            ("å°æ‰¹é‡æµ‹è¯•", result1),
            ("ä¸­ç­‰æ‰¹é‡æµ‹è¯•", result2),
            ("å¤§æ‰¹é‡æµ‹è¯•", result3)
        ]
        
        total_added = 0
        total_failed = 0
        
        for test_name, result in tests:
            if 'error' not in result:
                data = result.get('data', {})
                added = data.get('total_added', 0)
                failed = data.get('total_failed', 0)
                total_added += added
                total_failed += failed
                print(f"âœ… {test_name}: +{added} chunks (å¤±è´¥: {failed})")
            else:
                print(f"âŒ {test_name}: æµ‹è¯•å¤±è´¥")
        
        print(f"\nğŸ¯ æ€»è®¡:")
        print(f"   âœ… æˆåŠŸæ·»åŠ : {total_added} chunks")
        print(f"   âŒ å¤±è´¥æ•°é‡: {total_failed} chunks")
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {(total_added/(total_added+total_failed)*100):.1f}%" if (total_added + total_failed) > 0 else "N/A")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ KnowFlow æ‰¹é‡ Chunk æ·»åŠ  API æµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # é…ç½®å‚æ•° - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    BASE_URL = "http://8.134.177.47:15002"  # RAGFlow æœåŠ¡åœ°å€
    API_KEY = "ragflow-JmZjZlOGU2NWM4ZjExZjBhNGZmY2U4MD"  # ä½ çš„APIå¯†é’¥ï¼Œå¦‚æœéœ€è¦çš„è¯
    
    # æµ‹è¯•å‚æ•° - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    DATASET_ID = "c7a89db65e7211f0ade29e7ee8051cc1"      # æ›¿æ¢ä¸ºå®é™…çš„æ•°æ®é›†ID
    DOCUMENT_ID = "d4e0843a5e7211f08dd39e7ee8051cc1"    # æ›¿æ¢ä¸ºå®é™…çš„æ–‡æ¡£ID
    
    # æ£€æŸ¥å‚æ•°
    if DATASET_ID == "your_dataset_id_here" or DOCUMENT_ID == "your_document_id_here":
        print("âš ï¸  è¯·å…ˆé…ç½®æµ‹è¯•å‚æ•°!")
        print("è¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ä»¥ä¸‹å˜é‡:")
        print(f"   DATASET_ID = '{DATASET_ID}'")
        print(f"   DOCUMENT_ID = '{DOCUMENT_ID}'")
        print(f"   BASE_URL = '{BASE_URL}'")
        print(f"   API_KEY = '{API_KEY}' (å¦‚æœéœ€è¦)")
        print("\nğŸ’¡ è·å–IDçš„æ–¹æ³•:")
        print("   1. é€šè¿‡ RAGFlow Webç•Œé¢æŸ¥çœ‹URL")
        print("   2. é€šè¿‡ GET /api/v1/datasets è·å–æ•°æ®é›†åˆ—è¡¨")
        print("   3. é€šè¿‡ GET /api/v1/datasets/{dataset_id}/documents è·å–æ–‡æ¡£åˆ—è¡¨")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = KnowFlowBatchTester(BASE_URL, API_KEY)
    
    # è¯¢é—®æµ‹è¯•ç±»å‹
    print("\né€‰æ‹©æµ‹è¯•ç±»å‹:")
    print("1. è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ (æ¨è)")
    print("2. è‡ªå®šä¹‰å•æ¬¡æµ‹è¯•")
    print("3. æµ‹è¯•ä½ç½®ä¿¡æ¯åŠŸèƒ½ (æ–°å¢)")
    
    choice = input("è¯·é€‰æ‹© (1, 2 æˆ– 3): ").strip()
    
    if choice == "1":
        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        tester.run_test_suite(DATASET_ID, DOCUMENT_ID)
        
    elif choice == "2":
        # è‡ªå®šä¹‰æµ‹è¯•
        print("\nè‡ªå®šä¹‰æµ‹è¯•é…ç½®:")
        chunk_count = int(input("chunkæ•°é‡ (é»˜è®¤10): ") or 10)
        batch_size = int(input("æ‰¹é‡å¤§å° (é»˜è®¤5): ") or 5)
        content_prefix = input("å†…å®¹å‰ç¼€ (é»˜è®¤'è‡ªå®šä¹‰æµ‹è¯•'): ") or "è‡ªå®šä¹‰æµ‹è¯•"
        
        chunks = tester.create_test_chunks(chunk_count, content_prefix)
        result = tester.test_batch_add_chunks(DATASET_ID, DOCUMENT_ID, chunks, batch_size)
        
        if 'error' not in result:
            data = result.get('data', {})
            print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: æˆåŠŸæ·»åŠ  {data.get('total_added', 0)} chunks")
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {result['error']}")
            
    elif choice == "3":
        # æµ‹è¯•ä½ç½®ä¿¡æ¯åŠŸèƒ½
        print("\nğŸ§ª æµ‹è¯•ä½ç½®ä¿¡æ¯åŠŸèƒ½")
        print("=" * 40)
        
        print("\nğŸ“‹ æµ‹è¯•: åŒ…å«ä½ç½®ä¿¡æ¯çš„æ‰¹é‡æ·»åŠ ")
        print("-" * 40)
        position_chunks = tester.create_test_chunks_with_positions(3, "ä½ç½®æµ‹è¯•")
        result = tester.test_batch_add_chunks_with_positions(DATASET_ID, DOCUMENT_ID, position_chunks, batch_size=2)
        
        if 'error' not in result:
            data = result.get('data', {})
            print(f"\nğŸ¯ ä½ç½®ä¿¡æ¯æµ‹è¯•ç»“æœ:")
            print(f"   âœ… æˆåŠŸæ·»åŠ : {data.get('total_added', 0)} chunks")
            print(f"   ğŸ“ ä½ç½®åŠŸèƒ½: {'âœ… æ”¯æŒ' if any(c.get('positions') for c in data.get('chunks', [])) else 'âŒ ä¸æ”¯æŒ'}")
        else:
            print(f"\nâŒ ä½ç½®ä¿¡æ¯æµ‹è¯•å¤±è´¥: {result['error']}")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¿è¡Œå¹¶é€‰æ‹© 1, 2 æˆ– 3")


if __name__ == "__main__":
    main() 